{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "papermill": {
          "duration": 0.019692,
          "end_time": "2020-12-31T14:27:26.330653",
          "exception": false,
          "start_time": "2020-12-31T14:27:26.310961",
          "status": "completed"
        },
        "tags": [],
        "id": "rey2dArLMBFD"
      },
      "source": [
        "# 1. BART: Denoising Autoencoder for Pretraining Sequence-to-Sequence Models [Multi-Class Classifier]:\n",
        "### BART is trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text. It uses a standard Tranformer-based neural machine translation architecture which, despite its simplicity, can be seen as generalizing BERT (due to the bidirectional encoder), GPT (with the left-to-right decoder), and other recent pre- training schemes. BART is particularly effective when fine tuned for text generation but also works well for comprehension tasks. It matches the performance of RoBERTa on GLUE and SQuAD, and achieves new state-of-the-art results on a range of abstractive di-alogue, question answering, and summarization tasks, with gains of up to 3.5 ROUGE. BART also provides a 1.1 BLEU increase over a back-translation system for machine translation, with only target language pretraining. In the below task we utilize this pre-trained model for Zero-shot Classification.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "papermill": {
          "duration": 0.015088,
          "end_time": "2020-12-31T14:27:26.361166",
          "exception": false,
          "start_time": "2020-12-31T14:27:26.346078",
          "status": "completed"
        },
        "tags": [],
        "id": "iuRydmb8MBFE"
      },
      "source": [
        "## **Libraries/Dependencies**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lf14UaSkMTeG",
        "outputId": "726ed7d0-67ca-4790-a9eb-91be701ee0f1"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.31.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.16.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.3.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.7.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "execution": {
          "iopub.execute_input": "2020-12-31T14:27:26.398604Z",
          "iopub.status.busy": "2020-12-31T14:27:26.397993Z",
          "iopub.status.idle": "2020-12-31T14:27:29.075397Z",
          "shell.execute_reply": "2020-12-31T14:27:29.074333Z"
        },
        "papermill": {
          "duration": 2.69898,
          "end_time": "2020-12-31T14:27:29.075555",
          "exception": false,
          "start_time": "2020-12-31T14:27:26.376575",
          "status": "completed"
        },
        "tags": [],
        "id": "Ph6x4z7aMBFF"
      },
      "outputs": [],
      "source": [
        "# Import all the required libraries\n",
        "# Use Kaggle's pre-tuned notebooks to get the optimal versions of all the dependencies\n",
        "\n",
        "import nltk\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch.nn as nn\n",
        "# nltk.download('stopwords')\n",
        "from string import punctuation\n",
        "from collections import Counter\n",
        "from nltk.corpus import stopwords\n",
        "from torch.utils.data import TensorDataset, DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2020-12-31T14:27:29.118493Z",
          "iopub.status.busy": "2020-12-31T14:27:29.117784Z",
          "iopub.status.idle": "2020-12-31T14:29:16.945789Z",
          "shell.execute_reply": "2020-12-31T14:29:16.945046Z"
        },
        "papermill": {
          "duration": 107.853418,
          "end_time": "2020-12-31T14:29:16.945971",
          "exception": false,
          "start_time": "2020-12-31T14:27:29.092553",
          "status": "completed"
        },
        "tags": [],
        "id": "i5JAWsEJMBFG"
      },
      "outputs": [],
      "source": [
        "# Import all the required libraries\n",
        "from transformers import pipeline\n",
        "classifier = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2020-12-31T14:29:16.994197Z",
          "iopub.status.busy": "2020-12-31T14:29:16.993418Z",
          "iopub.status.idle": "2020-12-31T14:29:16.995903Z",
          "shell.execute_reply": "2020-12-31T14:29:16.996330Z"
        },
        "papermill": {
          "duration": 0.029708,
          "end_time": "2020-12-31T14:29:16.996447",
          "exception": false,
          "start_time": "2020-12-31T14:29:16.966739",
          "status": "completed"
        },
        "tags": [],
        "id": "sLAFL9wvMBFG"
      },
      "outputs": [],
      "source": [
        "# Create dataframe for final sentiment classification result\n",
        "def createDataFrame(labels, confidence, tweet):\n",
        "    labels = pd.DataFrame({'Labels': labels})\n",
        "    confidence = pd.DataFrame({'Confidence Scores': confidence})\n",
        "    column_values = ['Labels', 'Confidence']\n",
        "    sentiment_scores = pd.concat([labels,confidence], ignore_index=False, axis=1)\n",
        "    print(\"\\n--------------------------------------------------------------------------------------\")\n",
        "    print(f\"\\n Entered input sentence: {tweet}\")\n",
        "    print(\"\\n Sentiment of the tweet (Probability Distribution): \")\n",
        "    print(sentiment_scores.to_string(index=False))\n",
        "    #print(\"--------------------------------------------------------------------------------------\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2020-12-31T14:29:17.045377Z",
          "iopub.status.busy": "2020-12-31T14:29:17.044635Z",
          "iopub.status.idle": "2020-12-31T14:29:17.047776Z",
          "shell.execute_reply": "2020-12-31T14:29:17.047266Z"
        },
        "papermill": {
          "duration": 0.031062,
          "end_time": "2020-12-31T14:29:17.047867",
          "exception": false,
          "start_time": "2020-12-31T14:29:17.016805",
          "status": "completed"
        },
        "tags": [],
        "id": "D0DIANVmMBFG"
      },
      "outputs": [],
      "source": [
        "def sentiment_bart(tweet):\n",
        "    labels = []\n",
        "    confidence = []\n",
        "\n",
        "    # Possible Sentiment Categories\n",
        "    #candidate_labels = [\"happy\", \"sad\", \"warn\", \"angry\", \"sorrow\", \"alert\", \"neutral\"]\n",
        "    candidate_labels = [\"admiration\", \"amusement\", \"anger\", \"annoyance\", \"approval\", \"caring\", \"confusion\", \"curiosity\", \"desire\", \"disappointment\", \"disapproval\", \"disgust\", \"embarrassment\", \"excitement\", \"fear\", \"gratitude\", \"grief\", \"joy\", \"love\",  \"nervousness\", \"optimism\", \"pride\", \"realization\", \"relief\", \"remorse\", \"sadness\", \"surprise\", \"neutral\"]\n",
        "    #candidate_labels = [\"OYC\", \"DTC\", \"NCF\", \"KNY\", \"DCF\"]\n",
        "\n",
        "    # Send the labels and tweets to the classifier pipeline\n",
        "    result = classifier(tweet, candidate_labels)\n",
        "\n",
        "    # Extract the labels from results dictionary\n",
        "    labels.append(result[\"labels\"])\n",
        "    labels = [item for sublist in labels for item in sublist] # Flatten the list of lists into list\n",
        "\n",
        "    # Extract the labels from results dictionary\n",
        "    confidence.append(result[\"scores\"])\n",
        "    confidence = [(str(float(item)*100))[:6]+\" %\" for sublist in confidence for item in sublist] # Flatten the list of lists into list\n",
        "\n",
        "    createDataFrame(labels,confidence, tweet)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2020-12-31T14:29:17.095179Z",
          "iopub.status.busy": "2020-12-31T14:29:17.094533Z",
          "iopub.status.idle": "2020-12-31T14:29:19.593462Z",
          "shell.execute_reply": "2020-12-31T14:29:19.592888Z"
        },
        "papermill": {
          "duration": 2.525454,
          "end_time": "2020-12-31T14:29:19.593606",
          "exception": false,
          "start_time": "2020-12-31T14:29:17.068152",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ax5M1OGHMBFH",
        "outputId": "6dcdf1a7-a1d4-4fdb-e535-c1ec8da42d08"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Neural Sentiment Analysis of COVID-19 Tweets with BART\n",
            "\n",
            "------Available Options------\n",
            "1. Inference on Sample Tweets\n",
            "2. Enter Custom Tweets/Sentences\n",
            "3. Exit\n",
            "\n",
            "Please select an option from the above:\n",
            "\n",
            "--------------------------------------------------------------------------------------\n",
            "\n",
            " Entered input sentence: Many lost their jobs because of covid and it is highly dangerous\n",
            "\n",
            " Sentiment of the tweet (Probability Distribution): \n",
            "        Labels Confidence Scores\n",
            "disappointment          21.438 %\n",
            "   disapproval          12.346 %\n",
            "     confusion          8.9288 %\n",
            "   realization          6.8328 %\n",
            "       remorse          5.9936 %\n",
            "          fear          5.2666 %\n",
            "      surprise          4.1926 %\n",
            "   nervousness          3.8910 %\n",
            " embarrassment          3.5007 %\n",
            "       disgust          3.1420 %\n",
            "       sadness          2.6100 %\n",
            "         grief          2.3548 %\n",
            "        desire          2.1751 %\n",
            "     annoyance          1.9904 %\n",
            "    admiration          1.9645 %\n",
            "         pride          1.8590 %\n",
            "      approval          1.7790 %\n",
            "        caring          1.7165 %\n",
            "         anger          1.6198 %\n",
            "     curiosity          1.1488 %\n",
            "       neutral          1.0820 %\n",
            "    excitement          0.8276 %\n",
            "        relief          0.7873 %\n",
            "      optimism          0.6665 %\n",
            "     gratitude          0.6530 %\n",
            "     amusement          0.5098 %\n",
            "           joy          0.4548 %\n",
            "          love          0.2675 %\n",
            "\n",
            "--------------------------------------------------------------------------------------\n",
            "\n",
            " Entered input sentence: I am happy that my family members are safe in this tough times\n",
            "\n",
            " Sentiment of the tweet (Probability Distribution): \n",
            "        Labels Confidence Scores\n",
            "        relief          39.385 %\n",
            "     gratitude          11.965 %\n",
            "        caring          11.651 %\n",
            "      approval          6.2898 %\n",
            "      optimism          5.1034 %\n",
            "           joy          4.7846 %\n",
            "    admiration          3.7090 %\n",
            "   realization          2.7146 %\n",
            "      surprise          1.6754 %\n",
            "        desire          1.4969 %\n",
            "         pride          1.4377 %\n",
            "   disapproval          1.3148 %\n",
            "          love          1.1234 %\n",
            "     confusion          0.9563 %\n",
            "       neutral          0.8327 %\n",
            "    excitement          0.7328 %\n",
            "     amusement          0.7200 %\n",
            "       remorse          0.7169 %\n",
            "         grief          0.6838 %\n",
            "          fear          0.6517 %\n",
            "     curiosity          0.6480 %\n",
            "   nervousness          0.5956 %\n",
            "     annoyance          0.1760 %\n",
            "disappointment          0.1626 %\n",
            "         anger          0.1536 %\n",
            " embarrassment          0.1424 %\n",
            "       sadness          0.1222 %\n",
            "       disgust          0.0520 %\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nwhile(True):\\n    choice = int(input())\\n\\n    if choice == 1:\\n        sample_1 = \\'Many lost their jobs because of covid and it is highly dangerous\\'\\n        sentiment_bart(sample_1)\\n\\n        sample_2 = \\'I am happy that my family members are safe in this tough times\\'\\n        sentiment_bart(sample_2)\\n\\n    elif choice == 2:\\n        print(\"\\nPlease enter a sentence/tweet:\")\\n        user_input = input()\\n        sentiment_bart(user_input)\\n\\n    elif choice == 3:\\n        print(\"\\nExiting...\")\\n        break\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "# Driver program\n",
        "print(\"Neural Sentiment Analysis of COVID-19 Tweets with BART\")\n",
        "print(\"\\n------Available Options------\")\n",
        "print(\"1. Inference on Sample Tweets\")\n",
        "print(\"2. Enter Custom Tweets/Sentences\")\n",
        "print(\"3. Exit\")\n",
        "print(\"\\nPlease select an option from the above:\")\n",
        "\n",
        "\n",
        "sample_1 = 'Many lost their jobs because of covid and it is highly dangerous'\n",
        "sentiment_bart(sample_1)\n",
        "\n",
        "sample_2 = 'I am happy that my family members are safe in this tough times'\n",
        "sentiment_bart(sample_2)\n",
        "\n",
        "\"\"\"\n",
        "while(True):\n",
        "    choice = int(input())\n",
        "\n",
        "    if choice == 1:\n",
        "        sample_1 = 'Many lost their jobs because of covid and it is highly dangerous'\n",
        "        sentiment_bart(sample_1)\n",
        "\n",
        "        sample_2 = 'I am happy that my family members are safe in this tough times'\n",
        "        sentiment_bart(sample_2)\n",
        "\n",
        "    elif choice == 2:\n",
        "        print(\"\\nPlease enter a sentence/tweet:\")\n",
        "        user_input = input()\n",
        "        sentiment_bart(user_input)\n",
        "\n",
        "    elif choice == 3:\n",
        "        print(\"\\nExiting...\")\n",
        "        break\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "papermill": {
          "duration": 0.021799,
          "end_time": "2020-12-31T14:29:19.637862",
          "exception": false,
          "start_time": "2020-12-31T14:29:19.616063",
          "status": "completed"
        },
        "tags": [],
        "id": "jbtabTVNMBFH"
      },
      "source": [
        "# 2. Deep Long Short Term Memory Networks [Binary Classifier]:\n",
        "### Long short-term memory is an artificial recurrent neural network architecture used in the field of deep learning. Unlike standard feedforward neural networks, LSTM has feedback connections. It can not only process single data points, but also entire sequences of data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "papermill": {
          "duration": 0.021629,
          "end_time": "2020-12-31T14:29:19.681690",
          "exception": false,
          "start_time": "2020-12-31T14:29:19.660061",
          "status": "completed"
        },
        "tags": [],
        "id": "-wlRjCtzMBFH"
      },
      "source": [
        "## **Load Dataset & Initialize GPU**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2020-12-31T14:29:19.735322Z",
          "iopub.status.busy": "2020-12-31T14:29:19.734757Z",
          "iopub.status.idle": "2020-12-31T14:29:19.760436Z",
          "shell.execute_reply": "2020-12-31T14:29:19.759939Z"
        },
        "papermill": {
          "duration": 0.056992,
          "end_time": "2020-12-31T14:29:19.760556",
          "exception": false,
          "start_time": "2020-12-31T14:29:19.703564",
          "status": "completed"
        },
        "tags": [],
        "id": "O_LFeksUMBFH"
      },
      "outputs": [],
      "source": [
        "# Load the transfer learning tweet dataset\n",
        "sentiment_df = pd.read_csv('goemotions_3.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2020-12-31T14:29:19.809859Z",
          "iopub.status.busy": "2020-12-31T14:29:19.809151Z",
          "iopub.status.idle": "2020-12-31T14:29:19.813190Z",
          "shell.execute_reply": "2020-12-31T14:29:19.813850Z"
        },
        "papermill": {
          "duration": 0.031239,
          "end_time": "2020-12-31T14:29:19.813992",
          "exception": false,
          "start_time": "2020-12-31T14:29:19.782753",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YY-6KE9tMBFH",
        "outputId": "d31cb46c-4cfc-4a50-8345-c3c701c3324e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parallely Processing using CUDA\n"
          ]
        }
      ],
      "source": [
        "# Checking if NVIDIA Graphics Card and CUDA is available\n",
        "gpu_available = torch.cuda.is_available\n",
        "\n",
        "if gpu_available:\n",
        "    print('Parallely Processing using CUDA')\n",
        "else:\n",
        "    print('No CUDA Detected')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "papermill": {
          "duration": 0.022181,
          "end_time": "2020-12-31T14:29:19.859357",
          "exception": false,
          "start_time": "2020-12-31T14:29:19.837176",
          "status": "completed"
        },
        "tags": [],
        "id": "81vpUdA6MBFH"
      },
      "source": [
        "## **Pre-processing & Inference Module Definitions**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2020-12-31T14:29:19.919632Z",
          "iopub.status.busy": "2020-12-31T14:29:19.918794Z",
          "iopub.status.idle": "2020-12-31T14:29:19.920891Z",
          "shell.execute_reply": "2020-12-31T14:29:19.921815Z"
        },
        "papermill": {
          "duration": 0.040115,
          "end_time": "2020-12-31T14:29:19.921931",
          "exception": false,
          "start_time": "2020-12-31T14:29:19.881816",
          "status": "completed"
        },
        "tags": [],
        "id": "on8UYj8yMBFI"
      },
      "outputs": [],
      "source": [
        "# Pre-process the text and perform Stemming, Lemmatization and Stop-word removal\n",
        "def text_preprocessing(text):\n",
        "    remove_punctuation = [ch for ch in text if ch not in punctuation]\n",
        "    remove_punctuation = \"\".join(remove_punctuation).split()\n",
        "    filtered_text = [word.lower() for word in remove_punctuation if word.lower() not in stopwords.words('english')]\n",
        "    return filtered_text\n",
        "\n",
        "\n",
        "# Pad blank topken to keep the length of tweets consistent - mandatory to normalize and train the model\n",
        "def pad_features(reviews_int, seq_length):\n",
        "    features = np.zeros((len(reviews_int), seq_length), dtype=int)\n",
        "    for i, row in enumerate(reviews_int):\n",
        "        if len(row)!=0:\n",
        "            features[i, -len(row):] = np.array(row)[:seq_length]\n",
        "    return features\n",
        "\n",
        "# Convert the sentences into stream of tokens\n",
        "def tokenize(tweet):\n",
        "    test_ints = []\n",
        "    test_ints.append([vocab_to_int[word] for word in tweet])\n",
        "    return test_ints\n",
        "\n",
        "# Predict the sentiment of the tweet - performs binary classification using the model inference\n",
        "def sentiment(net, test_tweet, seq_length=50):\n",
        "    print(\"\\n--------------------------------------------------------------------------------------\")\n",
        "    print(f\"\\n Original input sentence: {test_tweet}\")\n",
        "    test_tweet = text_preprocessing(test_tweet)\n",
        "    tokenized_tweet = tokenize(test_tweet)\n",
        "\n",
        "    print(f\"\\n Pre-processed input sentence: {test_tweet}\")\n",
        "    #print(f\"\\nSentence converted into tokens:\\n{tokenized_tweet}\")\n",
        "\n",
        "    padded_tweet = pad_features(tokenized_tweet, 50)\n",
        "    feature_tensor = torch.from_numpy(padded_tweet)\n",
        "    batch_size = feature_tensor.size(0)\n",
        "\n",
        "    if gpu_available:\n",
        "        feature_tensor = feature_tensor.cuda()\n",
        "\n",
        "    h = net.init_hidden(batch_size)\n",
        "    output, h = net(feature_tensor, h)\n",
        "\n",
        "    print(output)\n",
        "\n",
        "    predicted_sentiment = torch.round(output.squeeze())\n",
        "    print(predicted_sentiment)\n",
        "    print('ok')\n",
        "\n",
        "    #if predicted_sentiment == 1:\n",
        "    #    print(\"\\n Sentiment: Positive\")\n",
        "\n",
        "    #else:\n",
        "    #    print(\"\\n Sentiment: Negative\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PoVgOiBWM431",
        "outputId": "4ab9c0cf-6290-4d9b-db47-797d5e14e77d"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2020-12-31T14:29:19.980033Z",
          "iopub.status.busy": "2020-12-31T14:29:19.974491Z",
          "iopub.status.idle": "2020-12-31T14:29:31.419370Z",
          "shell.execute_reply": "2020-12-31T14:29:31.418761Z"
        },
        "papermill": {
          "duration": 11.475027,
          "end_time": "2020-12-31T14:29:31.419485",
          "exception": false,
          "start_time": "2020-12-31T14:29:19.944458",
          "status": "completed"
        },
        "tags": [],
        "id": "dPMQo9UuMBFI"
      },
      "outputs": [],
      "source": [
        "# Code block to invoke Pre-processing, Padding and Tokenization operations on the tweet\n",
        "\n",
        "sentiment_df.loc[:, 'text'] = sentiment_df['text'].apply(text_preprocessing)\n",
        "\n",
        "reviews_split = []\n",
        "for i, j in sentiment_df.iterrows():\n",
        "    reviews_split.append(j['text'])\n",
        "\n",
        "words = []\n",
        "for review in reviews_split:\n",
        "    for word in review:\n",
        "        words.append(word)\n",
        "\n",
        "counts = Counter(words)\n",
        "vocab = sorted(counts, key=counts.get, reverse=True)\n",
        "vocab_to_int = {word:ii for ii, word in enumerate(vocab, 1)}\n",
        "\n",
        "encoded_reviews = []\n",
        "for review in reviews_split:\n",
        "    encoded_reviews.append([vocab_to_int[word] for word in review])\n",
        "\n",
        "labels_to_int = []\n",
        "for i, j in sentiment_df.iterrows():\n",
        "    if j['sentiment']=='admiration':\n",
        "        labels_to_int.append(1)\n",
        "    elif j['sentiment']=='amusement':\n",
        "        labels_to_int.append(2)\n",
        "    elif j['sentiment']=='anger':\n",
        "        labels_to_int.append(3)\n",
        "    elif j['sentiment']=='annoyance':\n",
        "        labels_to_int.append(4)\n",
        "    elif j['sentiment']=='approval':\n",
        "        labels_to_int.append(5)\n",
        "    elif j['sentiment']=='caring':\n",
        "        labels_to_int.append(6)\n",
        "    elif j['sentiment']=='confusion':\n",
        "        labels_to_int.append(7)\n",
        "    elif j['sentiment']=='curiosity':\n",
        "        labels_to_int.append(8)\n",
        "    elif j['sentiment']=='desire':\n",
        "        labels_to_int.append(9)\n",
        "    elif j['sentiment']=='disappointment':\n",
        "        labels_to_int.append(10)\n",
        "    elif j['sentiment']=='disapproval':\n",
        "        labels_to_int.append(11)\n",
        "    elif j['sentiment']=='disgust':\n",
        "        labels_to_int.append(12)\n",
        "    elif j['sentiment']=='embarrassment':\n",
        "        labels_to_int.append(13)\n",
        "    elif j['sentiment']=='excitement':\n",
        "        labels_to_int.append(14)\n",
        "    elif j['sentiment']=='fear':\n",
        "        labels_to_int.append(15)\n",
        "    elif j['sentiment']=='gratitude':\n",
        "        labels_to_int.append(16)\n",
        "    elif j['sentiment']=='grief':\n",
        "        labels_to_int.append(17)\n",
        "    elif j['sentiment']=='joy':\n",
        "        labels_to_int.append(18)\n",
        "    elif j['sentiment']=='love':\n",
        "        labels_to_int.append(19)\n",
        "    elif j['sentiment']=='nervousness':\n",
        "        labels_to_int.append(20)\n",
        "    elif j['sentiment']=='optimism':\n",
        "        labels_to_int.append(21)\n",
        "    elif j['sentiment']=='pride':\n",
        "        labels_to_int.append(22)\n",
        "    elif j['sentiment']=='realization':\n",
        "        labels_to_int.append(23)\n",
        "    elif j['sentiment']=='relief':\n",
        "        labels_to_int.append(24)\n",
        "    elif j['sentiment']=='remorse':\n",
        "        labels_to_int.append(25)\n",
        "    elif j['sentiment']=='sadness':\n",
        "        labels_to_int.append(26)\n",
        "    elif j['sentiment']=='surprise':\n",
        "        labels_to_int.append(27)\n",
        "    else:\n",
        "        labels_to_int.append(0)\n",
        "\n",
        "reviews_len = Counter([len(x) for x in encoded_reviews])\n",
        "non_zero_idx = [ii for ii, review in enumerate(encoded_reviews) if len(encoded_reviews)!=0]\n",
        "encoded_reviews = [encoded_reviews[ii] for ii in non_zero_idx]\n",
        "encoded_labels = np.array([labels_to_int[ii] for ii in non_zero_idx])\n",
        "\n",
        "seq_length = 50\n",
        "padded_features= pad_features(encoded_reviews, seq_length)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "papermill": {
          "duration": 0.022538,
          "end_time": "2020-12-31T14:29:31.465488",
          "exception": false,
          "start_time": "2020-12-31T14:29:31.442950",
          "status": "completed"
        },
        "tags": [],
        "id": "DGkSQPxGMBFI"
      },
      "source": [
        "## **Dataset and Dataloaders for Train, Test and Validation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2020-12-31T14:29:31.521233Z",
          "iopub.status.busy": "2020-12-31T14:29:31.520533Z",
          "iopub.status.idle": "2020-12-31T14:29:31.523563Z",
          "shell.execute_reply": "2020-12-31T14:29:31.524033Z"
        },
        "papermill": {
          "duration": 0.035943,
          "end_time": "2020-12-31T14:29:31.524140",
          "exception": false,
          "start_time": "2020-12-31T14:29:31.488197",
          "status": "completed"
        },
        "tags": [],
        "id": "2I4RjhDKMBFI"
      },
      "outputs": [],
      "source": [
        "# Split the dataset into Train (80%), Validation (10%) & Test (10%)\n",
        "batch_size = 1\n",
        "split_frac = 0.8\n",
        "split_idx = int(len(padded_features)*split_frac)\n",
        "\n",
        "training_x, remaining_x = padded_features[:split_idx], padded_features[split_idx:]\n",
        "training_y, remaining_y = encoded_labels[:split_idx], encoded_labels[split_idx:]\n",
        "\n",
        "test_idx = int(len(remaining_x)*0.5)\n",
        "val_x, test_x = remaining_x[:test_idx], remaining_x[test_idx:]\n",
        "val_y, test_y = remaining_y[:test_idx], remaining_y[test_idx:]\n",
        "\n",
        "# Transform the data into a Tensor datastructure\n",
        "train_data = TensorDataset(torch.from_numpy(training_x), torch.from_numpy(training_y))\n",
        "test_data = TensorDataset(torch.from_numpy(test_x), torch.from_numpy(test_y))\n",
        "valid_data = TensorDataset(torch.from_numpy(val_x), torch.from_numpy(val_y))\n",
        "\n",
        "# Prepare the dataloader for Train, Test and Validation\n",
        "train_loader = DataLoader(train_data, batch_size=batch_size)\n",
        "test_loader = DataLoader(test_data, batch_size=batch_size)\n",
        "valid_loader = DataLoader(valid_data, batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "papermill": {
          "duration": 0.022769,
          "end_time": "2020-12-31T14:29:31.569836",
          "exception": false,
          "start_time": "2020-12-31T14:29:31.547067",
          "status": "completed"
        },
        "tags": [],
        "id": "0GllsfkqMBFI"
      },
      "source": [
        "## **LSTM Model Architecture**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2020-12-31T14:29:31.620407Z",
          "iopub.status.busy": "2020-12-31T14:29:31.619841Z",
          "iopub.status.idle": "2020-12-31T14:29:31.623599Z",
          "shell.execute_reply": "2020-12-31T14:29:31.624045Z"
        },
        "papermill": {
          "duration": 0.031252,
          "end_time": "2020-12-31T14:29:31.624158",
          "exception": false,
          "start_time": "2020-12-31T14:29:31.592906",
          "status": "completed"
        },
        "tags": [],
        "id": "IbShuu2uMBFJ"
      },
      "outputs": [],
      "source": [
        "# Embedding Dimension of Tokens\n",
        "embedding_dim = 400\n",
        "\n",
        "# Embedding Dimension of Hidden Layers\n",
        "hidden_dim = 256\n",
        "\n",
        "# Output of the model is binary (either Positive or Negative)\n",
        "output_size = 1\n",
        "\n",
        "# Number of hidden LSTM cells\n",
        "n_layers = 2\n",
        "vocab_size = len(vocab_to_int)+1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2020-12-31T14:29:31.687037Z",
          "iopub.status.busy": "2020-12-31T14:29:31.686276Z",
          "iopub.status.idle": "2020-12-31T14:29:31.743253Z",
          "shell.execute_reply": "2020-12-31T14:29:31.742788Z"
        },
        "papermill": {
          "duration": 0.096054,
          "end_time": "2020-12-31T14:29:31.743349",
          "exception": false,
          "start_time": "2020-12-31T14:29:31.647295",
          "status": "completed"
        },
        "tags": [],
        "id": "ExzZYhGkMBFJ"
      },
      "outputs": [],
      "source": [
        "# Structure of the Neural Network\n",
        "class LSTM(nn.Module):\n",
        "    def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim, n_layers, drop_prob=0.2):\n",
        "        super(LSTM, self).__init__()\n",
        "        self.output_size = output_size\n",
        "        self.n_layers = n_layers\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.embedding_layer = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, dropout=drop_prob, batch_first=True)\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "        self.fc = nn.Linear(hidden_dim, output_size)\n",
        "        self.sig = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x, hidden):\n",
        "        batch_size = x.size(0)\n",
        "        x = x.long()\n",
        "        embeds = self.embedding_layer(x)\n",
        "        lstm_out, hidden = self.lstm(embeds, hidden)\n",
        "        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)\n",
        "        out = self.dropout(lstm_out)\n",
        "        out = self.fc(out)\n",
        "        sig_out = self.sig(out)\n",
        "        sig_out = sig_out.view(batch_size, -1)\n",
        "        sig_out = sig_out[:, -1]\n",
        "        return sig_out, hidden\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        weights = next(self.parameters()).data\n",
        "        if gpu_available:\n",
        "            hidden = (weights.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda(),weights.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda())\n",
        "        else:\n",
        "            hidden = (weights.new(self.n_layers, batch_size, self.hidden_dim).zero_(),weights.new(self.n_layers, batch_size, self.hidden_dim).zero())\n",
        "        return hidden\n",
        "\n",
        "net = LSTM(vocab_size, output_size, embedding_dim, hidden_dim, n_layers)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2020-12-31T14:29:31.797148Z",
          "iopub.status.busy": "2020-12-31T14:29:31.795377Z",
          "iopub.status.idle": "2020-12-31T14:29:31.797810Z",
          "shell.execute_reply": "2020-12-31T14:29:31.798265Z"
        },
        "papermill": {
          "duration": 0.031785,
          "end_time": "2020-12-31T14:29:31.798370",
          "exception": false,
          "start_time": "2020-12-31T14:29:31.766585",
          "status": "completed"
        },
        "tags": [],
        "id": "T01dkPZPMBFJ"
      },
      "outputs": [],
      "source": [
        "# Hyperparameters required for training of the network\n",
        "\n",
        "# Learning Rate\n",
        "lr = 0.001\n",
        "#lr = .0001\n",
        "\n",
        "# Loss Function - Binary Cross Entropy\n",
        "criterion = nn.BCELoss()\n",
        "\n",
        "# Gradient Descent based Optimizer - ADAM (Adaptive LR)\n",
        "optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
        "\n",
        "# Number of epochs to train the model\n",
        "epochs = 1\n",
        "count = 0\n",
        "\n",
        "# Step size\n",
        "#print_every = 200\n",
        "print_every = 2000\n",
        "clip = 5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "papermill": {
          "duration": 0.023238,
          "end_time": "2020-12-31T14:29:31.845068",
          "exception": false,
          "start_time": "2020-12-31T14:29:31.821830",
          "status": "completed"
        },
        "tags": [],
        "id": "7w2YhObIMBFJ"
      },
      "source": [
        "## **Model Training**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2020-12-31T14:29:31.908911Z",
          "iopub.status.busy": "2020-12-31T14:29:31.908239Z",
          "iopub.status.idle": "2020-12-31T14:31:57.523754Z",
          "shell.execute_reply": "2020-12-31T14:31:57.523117Z"
        },
        "papermill": {
          "duration": 145.655517,
          "end_time": "2020-12-31T14:31:57.523887",
          "exception": false,
          "start_time": "2020-12-31T14:29:31.868370",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ZJ7q3uLMBFJ",
        "outputId": "729acf55-197b-4417-ba23-ff43dbdc16f8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-25-ad355ca04592>:22: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(net.parameters(), clip)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1/1..... Step: 2000..... Train Loss: -100.000000...... Validation Loss: -200.000000\n",
            "Epoch: 1/1..... Step: 4000..... Train Loss: 100.000000...... Validation Loss: -200.000000\n",
            "Epoch: 1/1..... Step: 6000..... Train Loss: -1400.000000...... Validation Loss: -200.000000\n",
            "Epoch: 1/1..... Step: 8000..... Train Loss: -1500.000000...... Validation Loss: -200.000000\n",
            "Epoch: 1/1..... Step: 10000..... Train Loss: 100.000000...... Validation Loss: -200.000000\n",
            "Epoch: 1/1..... Step: 12000..... Train Loss: -600.000000...... Validation Loss: -200.000000\n",
            "Epoch: 1/1..... Step: 14000..... Train Loss: -300.000000...... Validation Loss: -200.000000\n",
            "Epoch: 1/1..... Step: 16000..... Train Loss: 100.000000...... Validation Loss: -200.000000\n",
            "Epoch: 1/1..... Step: 18000..... Train Loss: -700.000000...... Validation Loss: -200.000000\n",
            "Epoch: 1/1..... Step: 20000..... Train Loss: -1300.000000...... Validation Loss: -200.000000\n",
            "Epoch: 1/1..... Step: 22000..... Train Loss: -100.000000...... Validation Loss: -200.000000\n",
            "Epoch: 1/1..... Step: 24000..... Train Loss: 0.000000...... Validation Loss: -200.000000\n",
            "Epoch: 1/1..... Step: 26000..... Train Loss: 100.000000...... Validation Loss: -200.000000\n",
            "Epoch: 1/1..... Step: 28000..... Train Loss: 0.000000...... Validation Loss: -200.000000\n",
            "Epoch: 1/1..... Step: 30000..... Train Loss: -200.000000...... Validation Loss: -200.000000\n",
            "Epoch: 1/1..... Step: 32000..... Train Loss: -400.000000...... Validation Loss: -200.000000\n",
            "Epoch: 1/1..... Step: 34000..... Train Loss: 100.000000...... Validation Loss: -200.000000\n",
            "Epoch: 1/1..... Step: 36000..... Train Loss: -500.000000...... Validation Loss: -200.000000\n",
            "Epoch: 1/1..... Step: 38000..... Train Loss: -300.000000...... Validation Loss: -200.000000\n",
            "Epoch: 1/1..... Step: 40000..... Train Loss: -800.000000...... Validation Loss: -200.000000\n",
            "Epoch: 1/1..... Step: 42000..... Train Loss: -100.000000...... Validation Loss: -200.000000\n",
            "Epoch: 1/1..... Step: 44000..... Train Loss: 0.000000...... Validation Loss: -200.000000\n",
            "Epoch: 1/1..... Step: 46000..... Train Loss: 100.000000...... Validation Loss: -200.000000\n",
            "Epoch: 1/1..... Step: 48000..... Train Loss: -700.000000...... Validation Loss: -200.000000\n",
            "Epoch: 1/1..... Step: 50000..... Train Loss: -700.000000...... Validation Loss: -200.000000\n",
            "Epoch: 1/1..... Step: 52000..... Train Loss: -600.000000...... Validation Loss: -200.000000\n",
            "Epoch: 1/1..... Step: 54000..... Train Loss: -200.000000...... Validation Loss: -200.000000\n",
            "Epoch: 1/1..... Step: 56000..... Train Loss: 100.000000...... Validation Loss: -200.000000\n"
          ]
        }
      ],
      "source": [
        "# Train the Neural Network\n",
        "# Off-load the model to CUDA\n",
        "if gpu_available:\n",
        "    net.cuda()\n",
        "\n",
        "net.train()\n",
        "for e in range(epochs):\n",
        "    h = net.init_hidden(batch_size)\n",
        "\n",
        "    for inputs, labels in train_loader:\n",
        "        count += 1\n",
        "\n",
        "        if gpu_available:\n",
        "            inputs, labels = inputs.cuda(), labels.cuda()\n",
        "        h = tuple([each.data for each in h])\n",
        "\n",
        "        net.zero_grad()\n",
        "        outputs, h = net(inputs, h)\n",
        "        loss = criterion(outputs.squeeze(), labels.squeeze().float())\n",
        "\n",
        "        loss.backward()\n",
        "        nn.utils.clip_grad_norm(net.parameters(), clip)\n",
        "        optimizer.step()\n",
        "\n",
        "        if count % print_every == 0:\n",
        "            val_h = net.init_hidden(batch_size)\n",
        "            val_losses = []\n",
        "            net.eval()\n",
        "\n",
        "            for inputs, labels in valid_loader:\n",
        "                val_h = tuple([each.data for each in val_h])\n",
        "\n",
        "                if gpu_available:\n",
        "                    inputs, labels = inputs.cuda(), labels.cuda()\n",
        "\n",
        "            outputs, val_h = net(inputs, val_h)\n",
        "            val_loss = criterion(outputs.squeeze(), labels.squeeze().float())\n",
        "            val_losses.append(val_loss.item())\n",
        "\n",
        "            net.train()\n",
        "            print(f\"Epoch: {e+1}/{epochs}.....\",f\"Step: {count}.....\",\"Train Loss: {:.6f}......\".format(loss.item()),\"Validation Loss: {:.6f}\".format(np.mean(val_losses)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "papermill": {
          "duration": 0.056155,
          "end_time": "2020-12-31T14:31:57.637316",
          "exception": false,
          "start_time": "2020-12-31T14:31:57.581161",
          "status": "completed"
        },
        "tags": [],
        "id": "tujEeJsiMBFJ"
      },
      "source": [
        "## **Model Testing**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2020-12-31T14:31:57.761617Z",
          "iopub.status.busy": "2020-12-31T14:31:57.760970Z",
          "iopub.status.idle": "2020-12-31T14:31:58.397252Z",
          "shell.execute_reply": "2020-12-31T14:31:58.396091Z"
        },
        "papermill": {
          "duration": 0.703699,
          "end_time": "2020-12-31T14:31:58.397391",
          "exception": false,
          "start_time": "2020-12-31T14:31:57.693692",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MR-v2UsKMBFJ",
        "outputId": "7df775d6-16f1-4f5c-8c9b-40cecd7dc0bb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.08309537407195888\n",
            "0.08309537407195888\n",
            "0.08309537407195888\n",
            "0.08309537407195888\n",
            "Average Test Loss: -637.1359\n",
            "Average Test Accuracy: 0.0831\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
        "\n",
        "# Train the Neural Network\n",
        "test_losses = []\n",
        "num_correct = 0\n",
        "correct_labels = []\n",
        "preds = []\n",
        "\n",
        "h = net.init_hidden(batch_size)\n",
        "net.eval()\n",
        "\n",
        "for inputs, labels in test_loader:\n",
        "\n",
        "#    print('inputs:')\n",
        "#    print(inputs)\n",
        "#    print('labels:')\n",
        "#    print(labels)\n",
        "\n",
        "    h = tuple([each.data for each in h])\n",
        "\n",
        "    if gpu_available:\n",
        "        inputs, labels = inputs.cuda(), labels.cuda()\n",
        "\n",
        "    outputs, h = net(inputs, h)\n",
        "\n",
        "#    print(h)\n",
        "#    print(h.index(max(h)))\n",
        "\n",
        "#    print(inputs, labels, outputs)\n",
        "\n",
        "    test_loss = criterion(outputs.squeeze(), labels.squeeze().float())\n",
        "    test_losses.append(test_loss.item())\n",
        "\n",
        "    pred = torch.round(outputs.squeeze())\n",
        "#    pred = outputs.squeeze()\n",
        "    correct_tensor = pred.eq(labels.float().view_as(pred))\n",
        "\n",
        "#    print(labels.float()) #1,2,3...\n",
        "#    print(pred)\n",
        "\n",
        "#    print('preds:')\n",
        "#    print(len(preds))\n",
        "#    print('correct_tensor:')\n",
        "#    print(correct_tensor)\n",
        "\n",
        "    correct = np.squeeze(correct_tensor.numpy()) if not gpu_available else np.squeeze(correct_tensor.cpu().numpy())\n",
        "    num_correct += np.sum(correct)\n",
        "\n",
        "    preds.append(pred.cpu().detach())\n",
        "    correct_labels.append(labels.float().view_as(pred).cpu().detach())\n",
        "\n",
        "\"\"\"print('preds:')\n",
        "print(preds)\n",
        "print('dataset:')\n",
        "print(correct_labels)\"\"\"\n",
        "\n",
        "print(accuracy_score(correct_labels, preds))\n",
        "print(recall_score(correct_labels, preds, average='micro'))\n",
        "print(precision_score(correct_labels, preds, average='micro'))\n",
        "print(f1_score(correct_labels, preds, average='micro'))\n",
        "\n",
        "test_acc = num_correct/len(test_loader.dataset)\n",
        "\n",
        "print(\"Average Test Loss: {:.4f}\".format(np.mean(test_losses)))\n",
        "print(\"Average Test Accuracy: {:.4f}\".format(test_acc))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "papermill": {
          "duration": 0.057554,
          "end_time": "2020-12-31T14:31:58.516757",
          "exception": false,
          "start_time": "2020-12-31T14:31:58.459203",
          "status": "completed"
        },
        "tags": [],
        "id": "dI25AcWKMBFK"
      },
      "source": [
        "## **Main Program**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(net,'EmoBart_230812.pt')"
      ],
      "metadata": {
        "id": "gIySzUnbqTMf"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "net2 = torch.load('EmoBart_230812.pt')"
      ],
      "metadata": {
        "id": "G1gSqSF4q6zM"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
        "\n",
        "# Train the Neural Network\n",
        "test_losses = []\n",
        "num_correct = 0\n",
        "correct_labels = []\n",
        "preds = []\n",
        "\n",
        "h = net2.init_hidden(batch_size)\n",
        "net2.eval()\n",
        "\n",
        "for inputs, labels in test_loader:\n",
        "\n",
        "#    print('inputs:')\n",
        "#    print(inputs)\n",
        "#    print('labels:')\n",
        "#    print(labels)\n",
        "\n",
        "    h = tuple([each.data for each in h])\n",
        "\n",
        "    if gpu_available:\n",
        "        inputs, labels = inputs.cuda(), labels.cuda()\n",
        "\n",
        "    outputs, h = net2(inputs, h)\n",
        "\n",
        "#    print(h)\n",
        "#    print(h.index(max(h)))\n",
        "\n",
        "#    print(inputs, labels, outputs)\n",
        "\n",
        "    test_loss = criterion(outputs.squeeze(), labels.squeeze().float())\n",
        "    test_losses.append(test_loss.item())\n",
        "\n",
        "    pred = torch.round(outputs.squeeze())\n",
        "#    pred = outputs.squeeze()\n",
        "    correct_tensor = pred.eq(labels.float().view_as(pred))\n",
        "\n",
        "#    print(labels.float()) #1,2,3...\n",
        "#    print(pred)\n",
        "\n",
        "#    print('preds:')\n",
        "#    print(len(preds))\n",
        "#    print('correct_tensor:')\n",
        "#    print(correct_tensor)\n",
        "\n",
        "    correct = np.squeeze(correct_tensor.numpy()) if not gpu_available else np.squeeze(correct_tensor.cpu().numpy())\n",
        "    num_correct += np.sum(correct)\n",
        "\n",
        "    preds.append(pred.cpu().detach())\n",
        "    correct_labels.append(labels.float().view_as(pred).cpu().detach())\n",
        "\n",
        "\"\"\"print('preds:')\n",
        "print(preds)\n",
        "print('dataset:')\n",
        "print(correct_labels)\"\"\"\n",
        "\n",
        "print(accuracy_score(correct_labels, preds))\n",
        "print(recall_score(correct_labels, preds, average='micro'))\n",
        "print(precision_score(correct_labels, preds, average='micro'))\n",
        "print(f1_score(correct_labels, preds, average='micro'))\n",
        "\n",
        "test_acc = num_correct/len(test_loader.dataset)\n",
        "\n",
        "print(\"Average Test Loss: {:.4f}\".format(np.mean(test_losses)))\n",
        "print(\"Average Test Accuracy: {:.4f}\".format(test_acc))"
      ],
      "metadata": {
        "id": "M7KFU33mrB-U",
        "outputId": "77429b2b-9f6a-4a9e-d724-68a24d5fbd95",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.08309537407195888\n",
            "0.08309537407195888\n",
            "0.08309537407195888\n",
            "0.08309537407195888\n",
            "Average Test Loss: -637.1359\n",
            "Average Test Accuracy: 0.0831\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    },
    "papermill": {
      "duration": 277.779863,
      "end_time": "2020-12-31T14:32:00.068092",
      "environment_variables": {},
      "exception": null,
      "input_path": "__notebook__.ipynb",
      "output_path": "__notebook__.ipynb",
      "parameters": {},
      "start_time": "2020-12-31T14:27:22.288229",
      "version": "2.1.0"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}